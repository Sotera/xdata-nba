import json
import os
import logging
import urllib2
import re
from datetime import datetime
from string import Template
from bs4 import BeautifulSoup

""" NOTES
See notes on espn.py for general information.

We had to modify the existing espn scrapper to handle the data
for the 2012/2013 season which is using ESPN and JSON as oppose
to facebook and HTML
"""

""" Variables """
logging.basicConfig(filename='../../../espn.log', level=logging.DEBUG)
espnConvTemplate = Template("http://espn.go.com/nba/conversation?gameId=$gameId")
commentTemplate = Template("http://api.echoenabled.com/v1/search?xhr=1&q=childrenof%3Ahttp%3A%2F%2Fscores.espn.go.com%2Fnba%2Fconversation%3FgameId%3D$gameId+itemsPerPage%3A50+childrenItemsPerPage%3A50+&appkey=dev.espn.go.com")
outputDirPrefix = "../../../output/"
espnUrlFile = "%sespn_search_urls.txt" % outputDirPrefix
opener = urllib2.build_opener()
opener.addheaders = [('User-agent','Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')]
startTime = datetime.now()
gamesProcessed = 0
commentsProcessed = 0
exceptionsThrown = 0

print 'Starting Retrieval'
logging.info('Staring Retrieval')

""" Pull comments and replies out of the HTML element """
def processFeedback(commentRoot, commentsFile):
    global commentsProcessed
    commenterName = commentRoot['actor']['title']
    commenterComment = commentRoot['object']['content'].strip().replace('\n',' ')
    commentsFile.write("%s::%s\n" % (commenterName, commenterComment))
    commentsProcessed += 1
    """ TODO: all comments are included but to identify replies to existing comments
    the code will have to keep track of message IDs and Targets to accomplish this """

if os.path.exists(espnUrlFile):
    """ Read the URL file generated by the nba.com routine """
    with open(espnUrlFile, "r") as urlFile:
        for line in urlFile:
            try:
                url, query, nbaGameId = line.split('|')
                nbaGameId = nbaGameId.rstrip('\n')
                logging.debug("\n---ESPN Search URL: %s\n---Query: %s\n---GameId: %s" % (url, query, nbaGameId))
                unstructured = opener.open(url)
                soup = BeautifulSoup(unstructured)
                """ Follow the nba.com process file naming convention """
                commentsFileName = "%s00%s_espn_comments.txt" %(outputDirPrefix, nbaGameId)
                commentsFile = open(commentsFileName, 'w+')
                try:
                    """ Find the link that matches the game data """
                    for a in soup.find_all('a', text=re.compile(query), limit=1):
                        currentGameId = a['href'].split('=')[1]
                        commentUrl = commentTemplate.substitute(gameId=currentGameId)
                        logging.debug("Comment URL: %s" % commentUrl)
                        """ Poll the facebook comments page for the specific game """
                        response = opener.open(commentUrl)
                        comments = json.load(response, encoding='utf-8')
                        """ Iterate over the comments """
                        for e in comments['entries']:
                            try:
                                processFeedback(e, commentsFile)
                            except Exception as e:
                                logging.exception("Exception when processing comments")
                                exceptionsThrown += 1
                                """ Continue with remaining URLs """
                                continue
                except Exception as e:
                    logging.exception("Exception retrieving comments")
                    exceptionsThrown += 1
                finally:
                    commentsFile.close()
                gamesProcessed += 1
                print("."),
            except Exception as e:
                logging.exception("Unable to open URL: %s" % url)
                print("!"),
                exceptionsThrown += 1
                """ Continue with remaining URLs """
                continue
    urlFile.close()
opener.close()

endTime = datetime.now()
delta = endTime - startTime

print '\nStopping Retrieval\n-- Processing Time: %d seconds\n-- Games Processed: %s\n-- Comments Processed: %s\n-- Exceptions Caught: %s' %(delta.seconds + delta.microseconds/1E6, gamesProcessed, commentsProcessed, exceptionsThrown)
logging.info('Stopping Retrieval\n-- Processing Time: %d seconds\n-- Games Processed: %s\n-- Comments Processed: %s\n-- Exceptions Caught: %s' %(delta.seconds + delta.microseconds/1E6, gamesProcessed, commentsProcessed, exceptionsThrown))